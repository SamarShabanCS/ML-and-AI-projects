
- The idea of ensemble learning is closely related to the idea of the “wisdom of crowds“. This is where many different independent decisions, choices or estimates are combined into a final outcome that is often more accurate than any single contribution.
- In applied machine learning, it means combining the predictions from multiple models trained on your dataset, instead of seeking the single best performing model.
-The three main families of ensemble learning algorithms to choose from:
  - Bagging Ensembles

These ensembles involve training many high variance models on different samples of the training dataset.

Some algorithms include:

    Bagged Decision Trees
    Random Subspaces
    Random Forest
    Extra Trees
    Custom Bagging

- Boosting Ensembles

These ensembles involve adding models sequentially to correct the predictions of prior models.

Some algorithms include:

    AdaBoost
    Gradient Boosting Machine
    Stochastic Gradient Boosting
    XGBoost
    LightGBM

- Stacking Ensembles

These ensembles involve using a model to learn how to best combine the predictions from models.

Some algorithms include:

    Voting
    Weighted Average
    Blending
    Stacking
    Super Learner


